{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings, RAG and Vector Database"
      ],
      "metadata": {
        "id": "P8iz_wCX8BTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective:\n",
        "This notebook demonstrates the concepts of text embeddings, how a vector database stores and retrieves them, and how these are combined in a Retrieval Augmented Generation (RAG) pipeline to enhance an LLM's ability to answer questions based on specific knowledge."
      ],
      "metadata": {
        "id": "JDBhsFdnquQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Setup and Installation**"
      ],
      "metadata": {
        "id": "w-C-Zh2Qq9vk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qseQhj-Fprjf"
      },
      "outputs": [],
      "source": [
        "# install the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers faiss-cpu transformers numpy nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KfPlUK-HqUZI",
        "outputId": "a76139aa-f294-4d5e-dfb9-18bdf912257e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules"
      ],
      "metadata": {
        "id": "aVPxiqd0qYlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss\n",
        "import random # For simulating document chunks"
      ],
      "metadata": {
        "id": "8aIZq29BqanD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data for sentence tokenization"
      ],
      "metadata": {
        "id": "XnwCD9yUqc4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CTHvxFLdqfo0",
        "outputId": "18c2ae81-fd7f-4b19-ada5-e67ca4e1bf75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Text Embeddings**"
      ],
      "metadata": {
        "id": "d0bs2O9Tq0ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept:**\n",
        "Text embeddings are numerical representations (vectors) of text that capture semantic meaning. Texts with similar meanings will have vectors that are \"closer\" to each other in a multi-dimensional space.\n",
        "\n",
        "We'll use a pre-trained SentenceTransformer model to generate these embeddings."
      ],
      "metadata": {
        "id": "tp2rJ1evrKrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained sentence embedding model\n",
        "# 'all-MiniLM-L6-v2' is a good balance of performance and speed for demonstration\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Sample texts\n",
        "documents = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"Elephants are large mammals native to Africa and Asia.\",\n",
        "    \"Mount Everest is the highest mountain in the world.\",\n",
        "    \"What is the largest ocean on Earth? The Pacific Ocean.\"\n",
        "]\n",
        "\n",
        "print(\"Original Documents:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Doc {i+1}: {doc}\")\n",
        "\n",
        "# Generate embeddings for the documents\n",
        "document_embeddings = embedding_model.encode(documents)\n",
        "\n",
        "print(f\"\\nShape of embeddings: {document_embeddings.shape}\") # (num_documents, embedding_dimension)\n",
        "print(f\"Sample embedding for Doc 1 (first 5 dimensions): {document_embeddings[0][:5]}\")\n",
        "\n",
        "\n",
        "# You can observe that similar sentences will have closer embeddings.\n",
        "# Let's compare two related sentences and two unrelated sentences.\n",
        "query_related = \"What is the capital of France?\"\n",
        "query_unrelated = \"Tell me about cars.\"\n",
        "\n",
        "embedding_query_related = embedding_model.encode([query_related])\n",
        "embedding_query_unrelated = embedding_model.encode([query_unrelated])\n",
        "\n",
        "# Calculate cosine similarity (a common metric for vector similarity)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Similarity between query and related document\n",
        "sim_related = cosine_similarity(embedding_query_related, [document_embeddings[0]])\n",
        "print(f\"\\nCosine similarity between '{query_related}' and '{documents[0]}': {sim_related[0][0]:.4f}\")\n",
        "\n",
        "# Similarity between query and unrelated document\n",
        "sim_unrelated = cosine_similarity(embedding_query_unrelated, [document_embeddings[2]])\n",
        "print(f\"Cosine similarity between '{query_unrelated}' and '{documents[2]}': {sim_unrelated[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VFFE9RqVqhvU",
        "outputId": "0cbbec0a-7995-49f3-8594-dd0a4313bbfd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Documents:\n",
            "Doc 1: The capital of France is Paris.\n",
            "Doc 2: The Eiffel Tower is located in Paris.\n",
            "Doc 3: Elephants are large mammals native to Africa and Asia.\n",
            "Doc 4: Mount Everest is the highest mountain in the world.\n",
            "Doc 5: What is the largest ocean on Earth? The Pacific Ocean.\n",
            "\n",
            "Shape of embeddings: (5, 384)\n",
            "Sample embedding for Doc 1 (first 5 dimensions): [ 0.10325699  0.03042011  0.02909581 -0.0373229   0.07867623]\n",
            "\n",
            "Cosine similarity between 'What is the capital of France?' and 'The capital of France is Paris.': 0.8790\n",
            "Cosine similarity between 'Tell me about cars.' and 'Elephants are large mammals native to Africa and Asia.': 0.1869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Vector Database (using FAISS)**"
      ],
      "metadata": {
        "id": "9vR33QwP0M9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept:**\n",
        "\n",
        "A vector database is a specialized database designed to store, manage, and query high-dimensional vectors (embeddings). It allows for efficient \"similarity search,\" finding vectors that are most similar to a given query vector.\n",
        "\n",
        "We'll use FAISS (Facebook AI Similarity Search) as an in-memory vector database for simplicity."
      ],
      "metadata": {
        "id": "Gxhwv5pM0XY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dimension of the embeddings\n",
        "embedding_dimension = document_embeddings.shape[1]\n",
        "\n",
        "# Create a FAISS index (Flat Index with Inner Product similarity)\n",
        "# Inner Product is suitable when embeddings are normalized (which sentence-transformers often does)\n",
        "index = faiss.IndexFlatIP(embedding_dimension)\n",
        "\n",
        "# Add the document embeddings to the index\n",
        "index.add(document_embeddings)\n",
        "\n",
        "print(f\"\\nFAISS index created with {index.ntotal} documents.\")\n",
        "\n",
        "# Now, let's perform a similarity search.\n",
        "# We'll use the embedding of a query to find the most similar document.\n",
        "query = \"What is the highest peak in the world?\"\n",
        "query_embedding = embedding_model.encode([query])\n",
        "\n",
        "# Perform k-nearest neighbor search\n",
        "k = 2 # Retrieve top 2 similar documents\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "print(f\"\\nQuery: '{query}'\")\n",
        "print(f\"Top {k} similar documents:\")\n",
        "for i in range(k):\n",
        "    doc_index = indices[0][i]\n",
        "    distance = distances[0][i]\n",
        "    print(f\"  Rank {i+1}: Doc {doc_index+1} (Distance: {distance:.4f}) - '{documents[doc_index]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qfoYWFQz0a1B",
        "outputId": "35bd8fc8-7ca4-49f5-accc-78299bf3feb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAISS index created with 5 documents.\n",
            "\n",
            "Query: 'What is the highest peak in the world?'\n",
            "Top 2 similar documents:\n",
            "  Rank 1: Doc 4 (Distance: 0.6725) - 'Mount Everest is the highest mountain in the world.'\n",
            "  Rank 2: Doc 5 (Distance: 0.4135) - 'What is the largest ocean on Earth? The Pacific Ocean.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Retrieval Augmented Generation (RAG)**"
      ],
      "metadata": {
        "id": "d1eYhBcx0ftX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept:**\n",
        "RAG combines the strengths of retrieval systems (like vector databases) with generative large language models (LLMs). Instead of the LLM generating an answer solely from its pre-trained knowledge, RAG first retrieves relevant information from a knowledge base (using embeddings and a vector database) and then uses this retrieved information as context for the LLM to generate a more accurate and grounded answer.\n",
        "\n",
        "**Steps in RAG:**\n",
        "\n",
        "\n",
        "\n",
        "1. **Load Data & Chunking:** Load your knowledge base (e.g., a PDF, website text) and split it into smaller, manageable chunks.\n",
        "2.**Embed Chunks:** Generate embeddings for each chunk using an embedding model.\n",
        "3.**Store in Vector DB:** Store these chunk embeddings (and ideally the original chunks) in a vector database.\n",
        "4.**Query & Retrieve:** When a user asks a question, embed the query and use the vector database to find the most semantically similar chunks.\n",
        "5. **Augment Prompt:** Take the retrieved chunks and construct a prompt for the LLM that includes the original query and the retrieved context.\n",
        "6. **Generate Answer:** The LLM generates an answer based on the augmented prompt."
      ],
      "metadata": {
        "id": "qGMZmCFE0lcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4.1. Simulate a larger knowledge base (document splitting) ---\n",
        "long_text = \"\"\"\n",
        "The Amazon rainforest is the largest rainforest in the world, covering much of northwestern South America.\n",
        "It is home to an incredible diversity of wildlife, including jaguars, sloths, and countless species of birds and insects.\n",
        "The Amazon River flows through the rainforest and is the second-longest river in the world by length, and the largest by discharge volume.\n",
        "Deforestation is a major threat to the Amazon, impacting climate change and biodiversity.\n",
        "Sustainable practices are crucial for its preservation.\n",
        "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
        "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
        "It is often used for web development, data analysis, artificial intelligence, and scientific computing.\n",
        "\"\"\"\n",
        "\n",
        "# Simple chunking by sentences\n",
        "# For real-world applications, more sophisticated chunking strategies are needed\n",
        "# (e.g., recursive character text splitter from LangChain)\n",
        "\n",
        "# Download the required 'punkt_tab' resource for sentence tokenization\n",
        "import nltk\n",
        "try:\n",
        "    # Check if the resource exists\n",
        "    nltk.data.find('tokenizers/punkt_tab/')\n",
        "except LookupError:\n",
        "    # If it doesn't exist, download it\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "text_chunks = sent_tokenize(long_text)\n",
        "\n",
        "print(\"\\n--- 4.1. Document Chunks ---\")\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")\n",
        "\n",
        "# --- 4.2. Embed Chunks & 4.3. Store in Vector DB ---\n",
        "chunk_embeddings = embedding_model.encode(text_chunks)\n",
        "\n",
        "# Create a new FAISS index for these chunks\n",
        "chunk_embedding_dimension = chunk_embeddings.shape[1]\n",
        "chunk_index = faiss.IndexFlatIP(chunk_embedding_dimension)\n",
        "chunk_index.add(chunk_embeddings)\n",
        "print(\"\\nChunks Embeddings\")\n",
        "print(chunk_embeddings)\n",
        "\n",
        "print(f\"\\nFAISS index for chunks created with {chunk_index.ntotal} chunks.\")\n",
        "\n",
        "# --- 4.4. Query & Retrieve ---\n",
        "# Load a small, CPU-friendly LLM for demonstration\n",
        "# distilbert-base-cased-distilled-squad is a good choice for question-answering\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", tokenizer=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "def rag_pipeline(query, top_k_chunks=3):\n",
        "    \"\"\"\n",
        "    Implements a simplified RAG pipeline.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- RAG Pipeline for Query: '{query}' ---\")\n",
        "\n",
        "    # 1. Embed the query\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # 2. Retrieve relevant chunks from the vector database\n",
        "    distances, indices = chunk_index.search(query_embedding, top_k_chunks)\n",
        "    retrieved_chunk_indices = indices[0]\n",
        "\n",
        "    retrieved_documents = [text_chunks[i] for i in retrieved_chunk_indices]\n",
        "    print(\"\\nRetrieved Documents (from Vector Database):\")\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "        print(f\"  {i+1}. {doc}\")\n",
        "\n",
        "    # 3. Augment the prompt for the LLM\n",
        "    context = \" \".join(retrieved_documents)\n",
        "    # The LLM will use this context to answer the question.\n",
        "    # For a QA model, the 'context' parameter is directly used.\n",
        "    # For a generative LLM, you'd format a prompt like:\n",
        "    # prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    # 4. Generate answer using the LLM with augmented context\n",
        "    try:\n",
        "        answer = qa_pipeline(question=query, context=context)\n",
        "        print(\"\\nGenerated Answer (from LLM):\")\n",
        "        print(answer['answer'])\n",
        "        print(f\"Confidence Score: {answer['score']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during LLM generation: {e}\")\n",
        "        print(\"This might happen if the context is too long or the model struggles.\")\n",
        "        print(\"Consider a larger model or a more robust RAG framework like LangChain/LlamaIndex.\")\n",
        "\n",
        "# --- Test the RAG pipeline ---\n",
        "rag_pipeline(\"What is the Amazon rainforest known for?\")\n",
        "rag_pipeline(\"What programming paradigms does Python support?\")\n",
        "rag_pipeline(\"What is the main threat to the Amazon?\")\n",
        "rag_pipeline(\"What is the capital of Italy?\") # Example where the answer is NOT in the knowledge base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lunaOFHW6Wad",
        "outputId": "150d44fa-7367-4df2-819e-bf2b994c542c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 4.1. Document Chunks ---\n",
            "Chunk 1: \n",
            "The Amazon rainforest is the largest rainforest in the world, covering much of northwestern South America.\n",
            "Chunk 2: It is home to an incredible diversity of wildlife, including jaguars, sloths, and countless species of birds and insects.\n",
            "Chunk 3: The Amazon River flows through the rainforest and is the second-longest river in the world by length, and the largest by discharge volume.\n",
            "Chunk 4: Deforestation is a major threat to the Amazon, impacting climate change and biodiversity.\n",
            "Chunk 5: Sustainable practices are crucial for its preservation.\n",
            "Chunk 6: Python is a high-level, general-purpose programming language.\n",
            "Chunk 7: Its design philosophy emphasizes code readability with the use of significant indentation.\n",
            "Chunk 8: Python is dynamically typed and garbage-collected.\n",
            "Chunk 9: It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
            "Chunk 10: It is often used for web development, data analysis, artificial intelligence, and scientific computing.\n",
            "\n",
            "Chunks Embeddings\n",
            "[[ 1.36340737e-01 -7.16807088e-03 -9.80604818e-05 ... -4.58612554e-02\n",
            "  -6.29161624e-03  4.86976579e-02]\n",
            " [ 5.36830686e-02  1.72486957e-02 -3.87237733e-03 ...  1.51426317e-02\n",
            "  -1.35583207e-02  5.88886216e-02]\n",
            " [ 1.12428151e-01  2.48214584e-02 -7.82906916e-03 ... -8.26839358e-03\n",
            "   3.13268565e-02  4.19427790e-02]\n",
            " ...\n",
            " [-1.30992243e-02  1.78544968e-02 -3.03712934e-02 ...  7.91492835e-02\n",
            "   1.57128692e-01 -1.14138387e-02]\n",
            " [ 1.92929935e-02  9.92625393e-03 -6.16899915e-02 ...  1.28463628e-02\n",
            "   9.82438102e-02  1.12840505e-02]\n",
            " [-6.20415732e-02 -2.15965118e-02 -8.18582922e-02 ...  5.90503328e-02\n",
            "   9.47660133e-02 -7.93600548e-03]]\n",
            "\n",
            "FAISS index for chunks created with 10 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RAG Pipeline for Query: 'What is the Amazon rainforest known for?' ---\n",
            "\n",
            "Retrieved Documents (from Vector Database):\n",
            "  1. \n",
            "The Amazon rainforest is the largest rainforest in the world, covering much of northwestern South America.\n",
            "  2. Deforestation is a major threat to the Amazon, impacting climate change and biodiversity.\n",
            "  3. The Amazon River flows through the rainforest and is the second-longest river in the world by length, and the largest by discharge volume.\n",
            "\n",
            "Generated Answer (from LLM):\n",
            "the largest rainforest in the world\n",
            "Confidence Score: 0.3748\n",
            "\n",
            "--- RAG Pipeline for Query: 'What programming paradigms does Python support?' ---\n",
            "\n",
            "Retrieved Documents (from Vector Database):\n",
            "  1. Python is a high-level, general-purpose programming language.\n",
            "  2. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
            "  3. Python is dynamically typed and garbage-collected.\n",
            "\n",
            "Generated Answer (from LLM):\n",
            "structured (particularly procedural), object-oriented and functional programming\n",
            "Confidence Score: 0.7087\n",
            "\n",
            "--- RAG Pipeline for Query: 'What is the main threat to the Amazon?' ---\n",
            "\n",
            "Retrieved Documents (from Vector Database):\n",
            "  1. Deforestation is a major threat to the Amazon, impacting climate change and biodiversity.\n",
            "  2. \n",
            "The Amazon rainforest is the largest rainforest in the world, covering much of northwestern South America.\n",
            "  3. The Amazon River flows through the rainforest and is the second-longest river in the world by length, and the largest by discharge volume.\n",
            "\n",
            "Generated Answer (from LLM):\n",
            "Deforestation\n",
            "Confidence Score: 0.9919\n",
            "\n",
            "--- RAG Pipeline for Query: 'What is the capital of Italy?' ---\n",
            "\n",
            "Retrieved Documents (from Vector Database):\n",
            "  1. Python is a high-level, general-purpose programming language.\n",
            "  2. Python is dynamically typed and garbage-collected.\n",
            "  3. It is often used for web development, data analysis, artificial intelligence, and scientific computing.\n",
            "\n",
            "Generated Answer (from LLM):\n",
            "Python\n",
            "Confidence Score: 0.0616\n"
          ]
        }
      ]
    }
  ]
}